## 속도 줄이기
### 기존
![[스크린샷 2024-04-12 22.54.03.png]]
기존엔 출판사 별로 데이터를 불러오고 처리한 뒤에 역직렬화하는 함수 get_manga가 동기식이었다. 더군다나 이렇게 저장하고 다시 DB에서 불러올 때 수백 개의 데이터를 한꺼번에 불러오기 때문에 시간이 상당히 많이 소요됐다.
그래서 두 단계에 걸쳐 개선하기로 했다.
1. 비동기식으로 전환
2. 페이지네이션으로 한 페이지에 불러오는 데이터 양 줄이기
### 비동기식
함수의 로직은 다음과 같다.
* 반복문으로 출판사 별로 API를 요청하여 데이터 불러오기
* 반복문 안에서 데이터 처리
* 역직렬화

세 가지 로직을 하나의 함수에 담다 보니 쓸 데 없이 함수가 길어졌다. 그래서 로직대로 함수를 분리하기 했다.
* 데이터 가져오는 함수
* 가져온 데이터를 처리하고 역직렬화 및 DB 저장
* 위 과정을 출판사 별로 반복
세 가지 로직으로 나눈 뒤, 비동기식으로 함수를 작성했다.
```python
async def fetch_data(session, url):
	async with session.get(url) as response:
	if response.status == 200:
		return await response.json()
	else:
		print(f"Error: {response.status}")
		return None

```

```python
async def manga_to_db(session, publisher, existing_isbns):
    url = f"https://www.nl.go.kr/seoji/SearchApi.do?"

    API_KEY = settings.API_KEY
    result_style = "json"
    page_no = 1
    page_size = 200
    title = ""
    # start_publish_date, end_publish_date = calculate_next_month()
    start_publish_date, end_publish_date = "20240401", "20240430"

    params = {
        "cert_key": API_KEY,
        "result_style": result_style,
        "page_no": page_no,
        "page_size": page_size,
        "ebook_yn": "Y",
        "title": title,
        "start_publish_date": start_publish_date,
        "end_publish_date": end_publish_date,
        "publisher": publisher.search_keyword,
    }
    selected_data = []
    request_url = url + "&".join([f"{key}={value}" for key, value in params.items() if value])
    data = await fetch_data(session, request_url)

    if data:
        for manga in data["docs"]:
            if manga["EA_ISBN"] not in existing_isbns:
                if is_numeric(price := manga["PRE_PRICE"]) and manga["EA_ADD_CODE"] == publisher.ea_add_code:
                    author, illustrator, original_author, translator = parse_staff(manga["AUTHOR"])
                    manga_data = {
                        "title": manga["TITLE"],
                        "series_title": manga["SERIES_TITLE"],
                        "isSet": is_set(manga["TITLE"]),
                        "author": author,
                        "illustrator": illustrator,
                        "original_author": original_author,
                        "translator": translator,
                        "publisher": publisher.pk,
                        "published_at": datetime.strptime(manga["PUBLISH_PREDATE"], "%Y%m%d").date(),
                        "ea_isbn": manga["EA_ISBN"],
                        "price": int(price),
                    }
                    selected_data.append(manga_data)

        serializer = MangaCreateSerializer(data=selected_data, many=True)
        serializer.is_valid(raise_exception=True)
        serializer.save()
        return serializer.validated_data

```
```python
async def get_manga():
    publishers = Publisher.objects.all()
    existing_isbns = set(Manga.objects.values_list("ea_isbn", flat=True))

    async with aiohttp.ClientSession() as session:
        tasks = [manga_to_db(session, publisher, existing_isbns) for publisher in publishers]
        await asyncio.gather(*tasks)
```
비동기식 함수의 동작 방식은 아래 링크에서 공부하자
https://it-eldorado.tistory.com/159?category=749661

### 페이지네이션
DRF에서 페이지네이션을 적용하는 것 매우 쉽다. 단, generics 뷰나 ViewSet 한정...

#### 페이지네이션 클래스 정의
```python
class MangaPageNumberPagination(PageNumberPagination):
    page_size = 20
    page_size_query_param = "page_size"
    max_page_size = 100
```
#### 페이지네이션 적용
```python
class MangaListAPIView(generics.ListAPIView):
    queryset = Manga.objects.all().order_by("id")
    serializer_class = MangaModelSerializer
    pagination_class = MangaPageNumberPagination
```
pagination_class에 페이지네이션 클래스를 대입하면 끝.

### 검색
DRF에 검색을 적용하는 방법은 두 가지가 있다.
* DRF 내장 검색(필터링)
* django-filters 설치
ChatGPT는 DRF 내장 기능이 더 떨어진다고 하는데 일단 간단하게 구성할 땐 편의성 면에서 DRF 내장 기능이 더 편해서 둘 다 적용했다.